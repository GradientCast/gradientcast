{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# DensityAD Tuning Guide\n\nLearn how to configure detection parameters and filtering options.\n\n**Topics:**\n- Detection algorithm parameters\n- Post-processing filters\n- Sensitivity analysis\n- Production configuration"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from gradientcast import GradientCastDenseAD\n",
    "from utils.synthetic_data import generate_ad_payload_data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Replace with your API key\n",
    "GRADIENTCAST_API_KEY = \"your-api-key-here\"\n",
    "\n",
    "dense_ad = GradientCastDenseAD(api_key=GRADIENTCAST_API_KEY)\n",
    "\n",
    "# Generate test data\n",
    "data = generate_ad_payload_data(n_points=150, inject_anomalies=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Detection Parameters\n\n### Contamination\n\nThe expected proportion of anomalies in the dataset."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different contamination values\n",
    "contamination_values = [0.01, 0.05, 0.10, 0.15, 0.20]\n",
    "results = {}\n",
    "\n",
    "for cont in contamination_values:\n",
    "    result = dense_ad.detect(data, contamination=cont)\n",
    "    results[cont] = {\n",
    "        'raw': sum(1 for p in result.timeline if p.raw_anomaly),\n",
    "        'confirmed': len(result.anomalies)\n",
    "    }\n",
    "\n",
    "print(\"Contamination vs Detection:\")\n",
    "print(f\"{'Contamination':>12} | {'Raw':>6} | {'Confirmed':>10}\")\n",
    "print(\"-\" * 35)\n",
    "for cont, counts in results.items():\n",
    "    print(f\"{cont:>12.0%} | {counts['raw']:>6} | {counts['confirmed']:>10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize\nplt.figure(figsize=(10, 5))\nx = [c * 100 for c in contamination_values]\nraw = [results[c]['raw'] for c in contamination_values]\nconfirmed = [results[c]['confirmed'] for c in contamination_values]\n\nplt.plot(x, raw, 'b-o', label='Raw (initial detection)', markersize=8)\nplt.plot(x, confirmed, 'r-o', label='Confirmed (after filtering)', markersize=8)\n\nplt.xlabel('Contamination (%)')\nplt.ylabel('Anomalies Detected')\nplt.title('Impact of Contamination Parameter')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\nprint(\"\\nRecommendation: Start with 5% (0.05) and adjust based on false positive rate\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### N Neighbors\n\nNumber of neighbors for density calculation. Affects pattern sensitivity."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different n_neighbors values\n",
    "neighbor_values = [5, 10, 15, 20, 30, 40]\n",
    "neighbor_results = {}\n",
    "\n",
    "for n in neighbor_values:\n",
    "    result = dense_ad.detect(data, n_neighbors=n)\n",
    "    neighbor_results[n] = len(result.anomalies)\n",
    "\n",
    "print(\"N Neighbors vs Confirmed Anomalies:\")\n",
    "for n, count in neighbor_results.items():\n",
    "    print(f\"  n_neighbors={n:2d}: {count} anomalies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Post-Processing Filters\n",
    "\n",
    "### Valley Threshold\n",
    "\n",
    "Minimum value to consider for anomaly detection. Filters out noise on low-volume metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test valley threshold impact\n",
    "thresholds = [0, 100000, 500000, 1000000, 2000000]\n",
    "valley_results = {}\n",
    "\n",
    "for thresh in thresholds:\n",
    "    result = dense_ad.detect(data, valley_threshold=thresh)\n",
    "    valley_results[thresh] = len(result.anomalies)\n",
    "\n",
    "print(\"Valley Threshold vs Confirmed Anomalies:\")\n",
    "for thresh, count in valley_results.items():\n",
    "    print(f\"  threshold={thresh:>10,}: {count} anomalies\")\n",
    "\n",
    "print(\"\\nNote: Higher threshold = more points filtered as 'below meaningful volume'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum Contiguous Anomalies\n",
    "\n",
    "Requires N consecutive anomalies to confirm. Reduces isolated false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test contiguous requirement\n",
    "contiguous_values = [1, 2, 3, 4, 5]\n",
    "contiguous_results = {}\n",
    "\n",
    "for n in contiguous_values:\n",
    "    result = dense_ad.detect(data, min_contiguous_anomalies=n)\n",
    "    contiguous_results[n] = len(result.anomalies)\n",
    "\n",
    "print(\"Min Contiguous vs Confirmed Anomalies:\")\n",
    "for n, count in contiguous_results.items():\n",
    "    print(f\"  min_contiguous={n}: {count} anomalies\")\n",
    "\n",
    "print(\"\\nRecommendation: 2-3 for hourly data, reduces noise while catching real issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Combined Parameter Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for optimal parameters\n",
    "contamination_opts = [0.03, 0.05, 0.10]\n",
    "neighbor_opts = [10, 20, 30]\n",
    "contiguous_opts = [2, 3]\n",
    "\n",
    "print(\"Parameter Combinations:\")\n",
    "print(f\"{'Contamination':>12} | {'Neighbors':>9} | {'Contiguous':>10} | {'Anomalies':>9}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "best_config = None\n",
    "best_score = 0\n",
    "\n",
    "for cont in contamination_opts:\n",
    "    for neigh in neighbor_opts:\n",
    "        for contig in contiguous_opts:\n",
    "            result = dense_ad.detect(\n",
    "                data,\n",
    "                contamination=cont,\n",
    "                n_neighbors=neigh,\n",
    "                min_contiguous_anomalies=contig\n",
    "            )\n",
    "            count = len(result.anomalies)\n",
    "            print(f\"{cont:>12.0%} | {neigh:>9} | {contig:>10} | {count:>9}\")\n",
    "            \n",
    "            # Track configuration with moderate detection\n",
    "            if 3 <= count <= 10 and count > best_score:\n",
    "                best_score = count\n",
    "                best_config = (cont, neigh, contig)\n",
    "\n",
    "if best_config:\n",
    "    print(f\"\\nSuggested config: contamination={best_config[0]}, n_neighbors={best_config[1]}, min_contiguous={best_config[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Severity Thresholds\n",
    "\n",
    "Severity is determined by normalized score and z-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get result with default parameters\n",
    "result = dense_ad.detect(data)\n",
    "\n",
    "# Analyze severity distribution\n",
    "if result.anomalies:\n",
    "    print(\"Severity Analysis:\")\n",
    "    print(f\"{'Severity':>10} | {'Count':>6} | {'Avg Norm Score':>14} | {'Avg Z-Score':>12}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for sev in ['low', 'medium', 'high', 'critical']:\n",
    "        sev_points = [p for p in result.anomalies if p.magnitude.severity == sev]\n",
    "        if sev_points:\n",
    "            avg_norm = np.mean([p.magnitude.normalized_score for p in sev_points])\n",
    "            zscores = [p.magnitude.zscore_24h for p in sev_points if p.magnitude.zscore_24h]\n",
    "            avg_z = np.mean(zscores) if zscores else 0\n",
    "            print(f\"{sev:>10} | {len(sev_points):>6} | {avg_norm:>14.1f} | {avg_z:>12.2f}\")\n",
    "else:\n",
    "    print(\"No anomalies detected with current parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Configuration Examples\n",
    "\n",
    "### High Sensitivity (Catch More)\n",
    "\n",
    "```python\n",
    "result = dense_ad.detect(\n",
    "    data,\n",
    "    contamination=0.10,           # Higher contamination\n",
    "    n_neighbors=15,               # Fewer neighbors\n",
    "    min_contiguous_anomalies=1,   # No contiguous requirement\n",
    "    valley_threshold=100000       # Lower volume threshold\n",
    ")\n",
    "```\n",
    "\n",
    "### Balanced (Default-like)\n",
    "\n",
    "```python\n",
    "result = dense_ad.detect(\n",
    "    data,\n",
    "    contamination=0.05,\n",
    "    n_neighbors=20,\n",
    "    min_contiguous_anomalies=2\n",
    ")\n",
    "```\n",
    "\n",
    "### Low Sensitivity (Reduce Noise)\n",
    "\n",
    "```python\n",
    "result = dense_ad.detect(\n",
    "    data,\n",
    "    contamination=0.02,           # Lower contamination\n",
    "    n_neighbors=30,               # More neighbors\n",
    "    min_contiguous_anomalies=3,   # Require 3 consecutive\n",
    "    valley_threshold=2000000      # Higher volume threshold\n",
    ")\n",
    "```\n",
    "\n",
    "**Next:** [Real-time Simulation](03_simulation.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}