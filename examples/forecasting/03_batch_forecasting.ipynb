{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Forecasting & Performance\n",
    "\n",
    "Learn how to efficiently forecast multiple time series and understand performance characteristics.\n",
    "\n",
    "**Topics:**\n",
    "- Multi-series batch forecasting\n",
    "- Latency vs batch size analysis\n",
    "- Best practices for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gradientcast import GradientCastFM\n",
    "from utils.synthetic_data import generate_multivariate_series\n",
    "\n",
    "# Replace with your API key\n",
    "GRADIENTCAST_API_KEY = \"your-api-key-here\"\n",
    "\n",
    "fm = GradientCastFM(api_key=GRADIENTCAST_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Multi-Series Batch Forecasting\n",
    "\n",
    "Forecasting multiple series in a single API call is more efficient than individual calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(n_series, n_points=60):\n",
    "    \"\"\"Generate a batch of time series for testing.\"\"\"\n",
    "    data = generate_multivariate_series(\n",
    "        n_series=n_series,\n",
    "        n_points=n_points,\n",
    "        correlation=0.3,\n",
    "        noise_level=0.1\n",
    "    )\n",
    "    # Rename keys for clarity\n",
    "    return {f\"ts_{i}\": data[f\"series_{i}\"] for i in range(n_series)}\n",
    "\n",
    "# Generate batch of 10 series\n",
    "batch_data = generate_batch(n_series=10)\n",
    "\n",
    "# Forecast all at once\n",
    "start = time.time()\n",
    "result = fm.forecast(\n",
    "    input_data=batch_data,\n",
    "    horizon_len=12,\n",
    "    freq=\"M\"  # Monthly frequency\n",
    ")\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Batch of {len(batch_data)} series forecasted in {elapsed:.2f}s\")\n",
    "print(f\"Average per series: {elapsed/len(batch_data)*1000:.0f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Latency vs Batch Size\n",
    "\n",
    "Let's measure how latency scales with the number of series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different batch sizes\n",
    "batch_sizes = [1, 2, 4, 8, 16, 32]\n",
    "latencies = []\n",
    "per_series_latencies = []\n",
    "\n",
    "for n_series in batch_sizes:\n",
    "    batch_data = generate_batch(n_series=n_series)\n",
    "    \n",
    "    # Run 3 times and average\n",
    "    times = []\n",
    "    for _ in range(3):\n",
    "        start = time.time()\n",
    "        result = fm.forecast(\n",
    "            input_data=batch_data,\n",
    "            horizon_len=12,\n",
    "            freq=\"M\"\n",
    "        )\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    latencies.append(avg_time)\n",
    "    per_series_latencies.append(avg_time / n_series)\n",
    "    \n",
    "    print(f\"Batch size {n_series:2d}: {avg_time:.2f}s total, {avg_time/n_series*1000:.0f}ms per series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize latency scaling\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Total latency\n",
    "axes[0].plot(batch_sizes, latencies, 'bo-', markersize=8)\n",
    "axes[0].set_xlabel('Batch Size (# series)')\n",
    "axes[0].set_ylabel('Total Latency (seconds)')\n",
    "axes[0].set_title('Total Latency vs Batch Size')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Per-series latency\n",
    "axes[1].plot(batch_sizes, [l*1000 for l in per_series_latencies], 'ro-', markersize=8)\n",
    "axes[1].set_xlabel('Batch Size (# series)')\n",
    "axes[1].set_ylabel('Per-Series Latency (ms)')\n",
    "axes[1].set_title('Per-Series Latency vs Batch Size')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: Per-series latency decreases with larger batches (amortized overhead)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Context Length Impact\n",
    "\n",
    "Longer context (more history) generally improves accuracy but may increase latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different context lengths\n",
    "context_lengths = [24, 48, 96, 192]\n",
    "context_latencies = []\n",
    "\n",
    "for n_points in context_lengths:\n",
    "    batch_data = generate_batch(n_series=5, n_points=n_points)\n",
    "    \n",
    "    start = time.time()\n",
    "    result = fm.forecast(\n",
    "        input_data=batch_data,\n",
    "        horizon_len=12,\n",
    "        freq=\"H\"\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "    context_latencies.append(elapsed)\n",
    "    \n",
    "    print(f\"Context {n_points:3d} points: {elapsed:.2f}s\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(context_lengths, context_latencies, 'go-', markersize=8)\n",
    "plt.xlabel('Context Length (# points)')\n",
    "plt.ylabel('Latency (seconds)')\n",
    "plt.title('Latency vs Context Length')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Production Best Practices\n",
    "\n",
    "### 1. Batch Similar Series\n",
    "Group series with similar frequencies and context lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good: Batch series with same frequency\n",
    "daily_series = {\n",
    "    \"store_1\": generate_batch(1, 30)[\"ts_0\"],\n",
    "    \"store_2\": generate_batch(1, 30)[\"ts_0\"],\n",
    "    \"store_3\": generate_batch(1, 30)[\"ts_0\"],\n",
    "}\n",
    "\n",
    "result = fm.forecast(\n",
    "    input_data=daily_series,\n",
    "    horizon_len=7,\n",
    "    freq=\"D\"  # All daily\n",
    ")\n",
    "\n",
    "print(f\"Forecasted {len(daily_series)} series with same frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Handle Timeouts Gracefully\n",
    "\n",
    "Set appropriate timeouts based on batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradientcast import GradientCastFM, TimeoutError\n",
    "\n",
    "# For large batches, increase timeout\n",
    "fm_large = GradientCastFM(\n",
    "    api_key=GRADIENTCAST_API_KEY,\n",
    "    timeout=300  # 5 minutes for large workloads\n",
    ")\n",
    "\n",
    "try:\n",
    "    large_batch = generate_batch(n_series=50)\n",
    "    result = fm_large.forecast(\n",
    "        input_data=large_batch,\n",
    "        horizon_len=24,\n",
    "        freq=\"H\"\n",
    "    )\n",
    "    print(f\"Successfully forecasted {len(large_batch)} series\")\n",
    "except TimeoutError:\n",
    "    print(\"Consider splitting into smaller batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Monitor Processing Time\n",
    "\n",
    "Use response metadata to track performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = generate_batch(n_series=10)\n",
    "\n",
    "result = fm.forecast(\n",
    "    input_data=batch_data,\n",
    "    horizon_len=12,\n",
    "    freq=\"M\"\n",
    ")\n",
    "\n",
    "# Access processing metadata\n",
    "info = result.model_info\n",
    "print(f\"Server processing time: {info.processing_time:.3f}s\")\n",
    "print(f\"Context length used: {info.context_length}\")\n",
    "print(f\"Covariates used: {info.used_covariates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Recommendation | Why |\n",
    "|----------------|-----|\n",
    "| Batch 10-50 series | Optimal throughput vs latency |\n",
    "| Same frequency per batch | Consistent processing |\n",
    "| Increase timeout for large batches | Avoid premature failures |\n",
    "| Monitor `processing_time` | Track performance trends |\n",
    "\n",
    "**Next:** [Tuning Guide](tuning_guide.md) for optimization tips"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
